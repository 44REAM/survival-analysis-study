{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# For preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper \n",
    "\n",
    "from pycox.datasets import metabric\n",
    "from pycox.evaluation import EvalSurv\n",
    "from pycox.preprocessing.label_transforms import LabTransDiscreteTime\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_discrete_times = 10\n",
    "\n",
    "np.random.seed(1234)\n",
    "_ = torch.manual_seed(123)\n",
    "processor = LabTransDiscreteTime(n_discrete_times)\n",
    "\n",
    "\n",
    "df_train = metabric.read_df()\n",
    "df_test = df_train.sample(frac=0.2)\n",
    "df_train = df_train.drop(df_test.index)\n",
    "df_val = df_train.sample(frac=0.2)\n",
    "df_train = df_train.drop(df_val.index)\n",
    "\n",
    "# features scaling\n",
    "cols_standardize = ['x0', 'x1', 'x2', 'x3', 'x8']\n",
    "cols_leave = ['x4', 'x5', 'x6', 'x7']\n",
    "standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "leave = [(col, None) for col in cols_leave]\n",
    "x_mapper = DataFrameMapper(standardize + leave)\n",
    "x_train = x_mapper.fit_transform(df_train).astype('float32')\n",
    "x_val = x_mapper.transform(df_val).astype('float32')\n",
    "x_test = x_mapper.transform(df_test).astype('float32')\n",
    "\n",
    "# preprocessing time and event\n",
    "get_target = lambda df: (df['duration'].values, df['event'].values)\n",
    "y_train = processor.fit_transform(*get_target(df_train))\n",
    "y_val = processor.transform(*get_target(df_val))\n",
    "\n",
    "\n",
    "# We don't need to transform the test labels\n",
    "durations_test, events_test = get_target(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom deephit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.event = y[1]\n",
    "        self.duration = y[0]\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.duration[idx], self.event[idx]\n",
    "\n",
    "# implimentation of pair rank matrix for torch\n",
    "def pair_rank_mat_torch( idx_durations, events, dtype='float32'):\n",
    "    idx_durations = idx_durations.reshape(-1)\n",
    "    events = events.reshape(-1)\n",
    "    n = len(idx_durations)\n",
    "    mat = idx_durations.repeat(n, 1)\n",
    "\n",
    "    mat = (mat.T<mat)  | ((mat.T==mat) & (events.repeat(n, 1)==0))\n",
    "    mat = mat * events.repeat(n, 1).T\n",
    "\n",
    "    return mat.float()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(x_train, y_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=False, drop_last=True)\n",
    "\n",
    "val_dataset = MyDataset(x_val, y_val)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=256)\n",
    "\n",
    "test_dataset = MyDataset(x_test, (durations_test, events_test))\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x \n",
    "    \n",
    "def nll_pmf(phi: torch.Tensor, idx_durations: torch.Tensor, \n",
    "            events: torch.Tensor, epsilon: float = 1e-7) -> torch.Tensor:\n",
    "\n",
    "    if phi.shape[1] <= idx_durations.max():\n",
    "        raise ValueError(f\"Network output `phi` is too small for `idx_durations`.\"+\n",
    "                         f\" Need at least `phi.shape[1] = {idx_durations.max().item()+1}`,\"+\n",
    "                         f\" but got `phi.shape[1] = {phi.shape[1]}`\")\n",
    "    if events.dtype is torch.bool:\n",
    "        events = events.float()\n",
    "    events = events.view(-1)\n",
    " \n",
    "    idx_durations = idx_durations.view(-1, 1)\n",
    "\n",
    "    # pad for cumsum\n",
    "    pad = torch.zeros_like(phi[:, :1])\n",
    "    phi = torch.cat([pad, phi], dim=1)\n",
    "\n",
    "    # gamma for log-exp trick, not related to thoeretical derivation\n",
    "    gamma = phi.max(dim = 1)[0]\n",
    "    cumsum = phi.sub(gamma.view(-1, 1)).exp().cumsum(1)\n",
    "    sum_ = cumsum[:, -1]\n",
    "    \n",
    "    part1 = phi.gather(1, idx_durations).view(-1).sub(gamma).mul(events)\n",
    "    part2 = - sum_.relu().add(epsilon).log()\n",
    "    part3 = sum_.sub(cumsum.gather(1, idx_durations).view(-1)).relu().add(epsilon).log().mul(1. - events)\n",
    "    # need relu() in part3 (and possibly part2) because cumsum on gpu has some bugs and we risk getting negative numbers.\n",
    "    loss = - part1.add(part2).add(part3)\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def _diff_cdf_at_time_i(pmf: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "    n = pmf.shape[0]\n",
    "    ones = torch.ones((n, 1), device=pmf.device)\n",
    "\n",
    "    r = pmf.cumsum(1).matmul(y.transpose(0, 1))\n",
    "    diag_r = r.diag().view(1, -1)\n",
    "    r = ones.matmul(diag_r) - r\n",
    "    return r.transpose(0, 1)\n",
    "\n",
    "def rank_loss_deephit_single(phi: torch.Tensor, idx_durations: torch.Tensor, events: torch.Tensor, rank_mat: torch.Tensor,\n",
    "                             sigma: torch.Tensor, reduction: str = 'mean') -> torch.Tensor:\n",
    "\n",
    "    idx_durations = idx_durations.view(-1, 1)\n",
    " \n",
    "    pad = torch.zeros_like(phi[:, :1])\n",
    "    pmf = torch.cat([pad, phi], dim=1).softmax(1)\n",
    "\n",
    "    # pmf = utils.pad_col(phi).softmax(1)\n",
    "    # hit at the time point\n",
    "    y = torch.zeros_like(pmf).scatter(1, idx_durations, 1.) # one-hot\n",
    "\n",
    "    r = _diff_cdf_at_time_i(pmf, y)\n",
    "\n",
    "    rank_loss = rank_mat * torch.exp(-r/sigma)\n",
    "    rank_loss = rank_loss.mean(1, keepdim=True)\n",
    "    return rank_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 30\n",
    "sigma=0.1\n",
    "alpha = 0.2\n",
    "model = MyModel(x_train.shape[1], n_discrete_times)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.5758070349693298\n",
      "epoch: 1, loss: 0.5584962964057922\n",
      "epoch: 2, loss: 0.5462806820869446\n",
      "epoch: 3, loss: 0.5346958041191101\n",
      "epoch: 4, loss: 0.5252451300621033\n",
      "epoch: 5, loss: 0.5175309181213379\n",
      "epoch: 6, loss: 0.5105422735214233\n",
      "epoch: 7, loss: 0.5053091645240784\n",
      "epoch: 8, loss: 0.501338005065918\n",
      "epoch: 9, loss: 0.4980289041996002\n",
      "epoch: 10, loss: 0.4949811100959778\n",
      "epoch: 11, loss: 0.49251818656921387\n",
      "epoch: 12, loss: 0.49013611674308777\n",
      "epoch: 13, loss: 0.4879540205001831\n",
      "epoch: 14, loss: 0.4858100116252899\n",
      "epoch: 15, loss: 0.4837271571159363\n",
      "epoch: 16, loss: 0.4817905128002167\n",
      "epoch: 17, loss: 0.47983813285827637\n",
      "epoch: 18, loss: 0.4781160056591034\n",
      "epoch: 19, loss: 0.4762911796569824\n",
      "epoch: 20, loss: 0.4744130074977875\n",
      "epoch: 21, loss: 0.4725854992866516\n",
      "epoch: 22, loss: 0.47066834568977356\n",
      "epoch: 23, loss: 0.46893563866615295\n",
      "epoch: 24, loss: 0.46720772981643677\n",
      "epoch: 25, loss: 0.46553540229797363\n",
      "epoch: 26, loss: 0.46393322944641113\n",
      "epoch: 27, loss: 0.462224543094635\n",
      "epoch: 28, loss: 0.46066713333129883\n",
      "epoch: 29, loss: 0.45907509326934814\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    losses = []\n",
    "    for x, duration, event in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        nll = nll_pmf(output, duration, event)\n",
    "        rank_mat = pair_rank_mat_torch(duration, event)\n",
    "        rank_loss = rank_loss_deephit_single(output, duration, event, rank_mat, 2)\n",
    "        loss = alpha * nll + (1. - alpha) * rank_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "    print('epoch: {}, loss: {}'.format(i, np.mean(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "durations = []\n",
    "events = []\n",
    "for x, duration, event in test_dataloader:\n",
    "    output = model(x)\n",
    "    outputs.append(output)\n",
    "    durations.append(duration)\n",
    "    events.append(event)\n",
    "\n",
    "outputs = torch.cat(outputs, dim=0)\n",
    "surv = outputs.cpu().detach().numpy()\n",
    "durations = torch.cat(durations, dim=0).cpu().detach().numpy()\n",
    "events = torch.cat(events, dim=0).cpu().detach().numpy()\n",
    "surv_df = pd.DataFrame(-surv, columns=processor.cuts).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = EvalSurv(surv_df, durations, events, censor_surv='km')\n",
    "final.concordance_td('antolini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try interpolation (not use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "all_risk = []\n",
    "for i in range(len(surv)):\n",
    "    spl = CubicSpline(processor.cuts, surv[i])\n",
    "    risks = []\n",
    "    for j in range(381):\n",
    "        risks.append(spl(j).item())\n",
    "    all_risk.append(risks)\n",
    "all_risk = np.array(all_risk).cumsum(axis= 0).T\n",
    "all_risk = pd.DataFrame(all_risk, columns=range(381))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deephit from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[4s / 4s],\t\ttrain_loss: 2.3483,\tval_loss: 7.2094\n",
      "1:\t[0s / 4s],\t\ttrain_loss: 1.9862,\tval_loss: 6.6771\n",
      "2:\t[0s / 4s],\t\ttrain_loss: 1.8407,\tval_loss: 6.2297\n",
      "3:\t[0s / 4s],\t\ttrain_loss: 1.6534,\tval_loss: 5.7992\n",
      "4:\t[0s / 4s],\t\ttrain_loss: 1.5680,\tval_loss: 5.3871\n",
      "5:\t[0s / 4s],\t\ttrain_loss: 1.4264,\tval_loss: 5.0353\n",
      "6:\t[0s / 4s],\t\ttrain_loss: 1.2725,\tval_loss: 4.7366\n",
      "7:\t[0s / 5s],\t\ttrain_loss: 1.2169,\tval_loss: 4.4896\n",
      "8:\t[0s / 5s],\t\ttrain_loss: 1.1433,\tval_loss: 4.2741\n",
      "9:\t[0s / 5s],\t\ttrain_loss: 1.0449,\tval_loss: 4.0920\n",
      "10:\t[0s / 5s],\t\ttrain_loss: 1.0155,\tval_loss: 3.9349\n",
      "11:\t[0s / 5s],\t\ttrain_loss: 0.9111,\tval_loss: 3.7965\n",
      "12:\t[0s / 5s],\t\ttrain_loss: 0.9212,\tval_loss: 3.6782\n",
      "13:\t[0s / 5s],\t\ttrain_loss: 0.9229,\tval_loss: 3.5734\n",
      "14:\t[0s / 5s],\t\ttrain_loss: 0.8663,\tval_loss: 3.4832\n",
      "15:\t[0s / 5s],\t\ttrain_loss: 0.8197,\tval_loss: 3.4005\n",
      "16:\t[0s / 5s],\t\ttrain_loss: 0.7995,\tval_loss: 3.3249\n",
      "17:\t[0s / 5s],\t\ttrain_loss: 0.8022,\tval_loss: 3.2530\n",
      "18:\t[0s / 5s],\t\ttrain_loss: 0.7919,\tval_loss: 3.1921\n",
      "19:\t[0s / 5s],\t\ttrain_loss: 0.7721,\tval_loss: 3.1353\n",
      "20:\t[0s / 5s],\t\ttrain_loss: 0.7533,\tval_loss: 3.0832\n",
      "21:\t[0s / 5s],\t\ttrain_loss: 0.7721,\tval_loss: 3.0375\n",
      "22:\t[0s / 5s],\t\ttrain_loss: 0.7359,\tval_loss: 2.9968\n",
      "23:\t[0s / 6s],\t\ttrain_loss: 0.7314,\tval_loss: 2.9597\n",
      "24:\t[0s / 6s],\t\ttrain_loss: 0.6956,\tval_loss: 2.9261\n",
      "25:\t[0s / 6s],\t\ttrain_loss: 0.7162,\tval_loss: 2.8933\n",
      "26:\t[0s / 6s],\t\ttrain_loss: 0.6962,\tval_loss: 2.8635\n",
      "27:\t[0s / 6s],\t\ttrain_loss: 0.6929,\tval_loss: 2.8352\n",
      "28:\t[0s / 6s],\t\ttrain_loss: 0.6884,\tval_loss: 2.8072\n",
      "29:\t[0s / 6s],\t\ttrain_loss: 0.6666,\tval_loss: 2.7815\n",
      "30:\t[0s / 6s],\t\ttrain_loss: 0.6662,\tval_loss: 2.7575\n",
      "31:\t[0s / 6s],\t\ttrain_loss: 0.6584,\tval_loss: 2.7343\n",
      "32:\t[0s / 6s],\t\ttrain_loss: 0.6480,\tval_loss: 2.7121\n",
      "33:\t[0s / 6s],\t\ttrain_loss: 0.6454,\tval_loss: 2.6904\n",
      "34:\t[0s / 6s],\t\ttrain_loss: 0.6409,\tval_loss: 2.6698\n",
      "35:\t[0s / 6s],\t\ttrain_loss: 0.6479,\tval_loss: 2.6512\n",
      "36:\t[0s / 6s],\t\ttrain_loss: 0.6286,\tval_loss: 2.6319\n",
      "37:\t[0s / 6s],\t\ttrain_loss: 0.6293,\tval_loss: 2.6132\n",
      "38:\t[0s / 6s],\t\ttrain_loss: 0.6167,\tval_loss: 2.5943\n",
      "39:\t[0s / 6s],\t\ttrain_loss: 0.6216,\tval_loss: 2.5770\n",
      "40:\t[0s / 7s],\t\ttrain_loss: 0.6170,\tval_loss: 2.5610\n",
      "41:\t[0s / 7s],\t\ttrain_loss: 0.6142,\tval_loss: 2.5445\n",
      "42:\t[0s / 7s],\t\ttrain_loss: 0.6078,\tval_loss: 2.5293\n",
      "43:\t[0s / 7s],\t\ttrain_loss: 0.6033,\tval_loss: 2.5139\n",
      "44:\t[0s / 7s],\t\ttrain_loss: 0.6018,\tval_loss: 2.4993\n",
      "45:\t[0s / 7s],\t\ttrain_loss: 0.6058,\tval_loss: 2.4849\n",
      "46:\t[0s / 7s],\t\ttrain_loss: 0.5970,\tval_loss: 2.4711\n",
      "47:\t[0s / 7s],\t\ttrain_loss: 0.5888,\tval_loss: 2.4580\n",
      "48:\t[0s / 7s],\t\ttrain_loss: 0.5866,\tval_loss: 2.4443\n",
      "49:\t[0s / 7s],\t\ttrain_loss: 0.5854,\tval_loss: 2.4323\n",
      "50:\t[0s / 7s],\t\ttrain_loss: 0.5889,\tval_loss: 2.4199\n",
      "51:\t[0s / 7s],\t\ttrain_loss: 0.5844,\tval_loss: 2.4086\n",
      "52:\t[0s / 7s],\t\ttrain_loss: 0.5849,\tval_loss: 2.3960\n",
      "53:\t[0s / 7s],\t\ttrain_loss: 0.5755,\tval_loss: 2.3837\n",
      "54:\t[0s / 7s],\t\ttrain_loss: 0.5829,\tval_loss: 2.3710\n",
      "55:\t[0s / 7s],\t\ttrain_loss: 0.5732,\tval_loss: 2.3588\n",
      "56:\t[0s / 8s],\t\ttrain_loss: 0.5719,\tval_loss: 2.3471\n",
      "57:\t[0s / 8s],\t\ttrain_loss: 0.5658,\tval_loss: 2.3365\n",
      "58:\t[0s / 8s],\t\ttrain_loss: 0.5752,\tval_loss: 2.3255\n",
      "59:\t[0s / 8s],\t\ttrain_loss: 0.5762,\tval_loss: 2.3146\n",
      "60:\t[0s / 8s],\t\ttrain_loss: 0.5674,\tval_loss: 2.3044\n",
      "61:\t[0s / 8s],\t\ttrain_loss: 0.5622,\tval_loss: 2.2955\n",
      "62:\t[0s / 8s],\t\ttrain_loss: 0.5575,\tval_loss: 2.2853\n",
      "63:\t[0s / 8s],\t\ttrain_loss: 0.5604,\tval_loss: 2.2755\n",
      "64:\t[0s / 8s],\t\ttrain_loss: 0.5581,\tval_loss: 2.2644\n",
      "65:\t[0s / 8s],\t\ttrain_loss: 0.5569,\tval_loss: 2.2547\n",
      "66:\t[0s / 8s],\t\ttrain_loss: 0.5540,\tval_loss: 2.2445\n",
      "67:\t[0s / 8s],\t\ttrain_loss: 0.5592,\tval_loss: 2.2355\n",
      "68:\t[0s / 8s],\t\ttrain_loss: 0.5516,\tval_loss: 2.2251\n",
      "69:\t[0s / 8s],\t\ttrain_loss: 0.5558,\tval_loss: 2.2155\n",
      "70:\t[0s / 8s],\t\ttrain_loss: 0.5551,\tval_loss: 2.2060\n",
      "71:\t[0s / 8s],\t\ttrain_loss: 0.5523,\tval_loss: 2.1981\n",
      "72:\t[0s / 9s],\t\ttrain_loss: 0.5491,\tval_loss: 2.1896\n",
      "73:\t[0s / 9s],\t\ttrain_loss: 0.5493,\tval_loss: 2.1821\n",
      "74:\t[0s / 9s],\t\ttrain_loss: 0.5459,\tval_loss: 2.1735\n",
      "75:\t[0s / 9s],\t\ttrain_loss: 0.5461,\tval_loss: 2.1658\n",
      "76:\t[0s / 9s],\t\ttrain_loss: 0.5441,\tval_loss: 2.1583\n",
      "77:\t[0s / 9s],\t\ttrain_loss: 0.5455,\tval_loss: 2.1504\n",
      "78:\t[0s / 9s],\t\ttrain_loss: 0.5444,\tval_loss: 2.1417\n",
      "79:\t[0s / 9s],\t\ttrain_loss: 0.5433,\tval_loss: 2.1337\n",
      "80:\t[0s / 9s],\t\ttrain_loss: 0.5400,\tval_loss: 2.1245\n",
      "81:\t[0s / 9s],\t\ttrain_loss: 0.5362,\tval_loss: 2.1179\n",
      "82:\t[0s / 9s],\t\ttrain_loss: 0.5448,\tval_loss: 2.1111\n",
      "83:\t[0s / 9s],\t\ttrain_loss: 0.5369,\tval_loss: 2.1035\n",
      "84:\t[0s / 9s],\t\ttrain_loss: 0.5372,\tval_loss: 2.0970\n",
      "85:\t[0s / 9s],\t\ttrain_loss: 0.5355,\tval_loss: 2.0898\n",
      "86:\t[0s / 9s],\t\ttrain_loss: 0.5370,\tval_loss: 2.0818\n",
      "87:\t[0s / 9s],\t\ttrain_loss: 0.5331,\tval_loss: 2.0752\n",
      "88:\t[0s / 10s],\t\ttrain_loss: 0.5344,\tval_loss: 2.0679\n",
      "89:\t[0s / 10s],\t\ttrain_loss: 0.5313,\tval_loss: 2.0615\n",
      "90:\t[0s / 10s],\t\ttrain_loss: 0.5302,\tval_loss: 2.0544\n",
      "91:\t[0s / 10s],\t\ttrain_loss: 0.5301,\tval_loss: 2.0479\n",
      "92:\t[0s / 10s],\t\ttrain_loss: 0.5321,\tval_loss: 2.0418\n",
      "93:\t[0s / 10s],\t\ttrain_loss: 0.5268,\tval_loss: 2.0348\n",
      "94:\t[0s / 10s],\t\ttrain_loss: 0.5257,\tval_loss: 2.0279\n",
      "95:\t[0s / 10s],\t\ttrain_loss: 0.5311,\tval_loss: 2.0210\n",
      "96:\t[0s / 10s],\t\ttrain_loss: 0.5255,\tval_loss: 2.0146\n",
      "97:\t[0s / 10s],\t\ttrain_loss: 0.5291,\tval_loss: 2.0082\n",
      "98:\t[0s / 10s],\t\ttrain_loss: 0.5249,\tval_loss: 2.0032\n",
      "99:\t[0s / 10s],\t\ttrain_loss: 0.5229,\tval_loss: 1.9969\n",
      "100:\t[0s / 10s],\t\ttrain_loss: 0.5229,\tval_loss: 1.9896\n",
      "101:\t[0s / 10s],\t\ttrain_loss: 0.5259,\tval_loss: 1.9832\n",
      "102:\t[0s / 10s],\t\ttrain_loss: 0.5238,\tval_loss: 1.9782\n",
      "103:\t[0s / 10s],\t\ttrain_loss: 0.5226,\tval_loss: 1.9728\n",
      "104:\t[0s / 11s],\t\ttrain_loss: 0.5216,\tval_loss: 1.9667\n",
      "105:\t[0s / 11s],\t\ttrain_loss: 0.5253,\tval_loss: 1.9612\n",
      "106:\t[0s / 11s],\t\ttrain_loss: 0.5213,\tval_loss: 1.9553\n",
      "107:\t[0s / 11s],\t\ttrain_loss: 0.5225,\tval_loss: 1.9489\n",
      "108:\t[0s / 11s],\t\ttrain_loss: 0.5194,\tval_loss: 1.9424\n",
      "109:\t[0s / 11s],\t\ttrain_loss: 0.5201,\tval_loss: 1.9362\n",
      "110:\t[0s / 11s],\t\ttrain_loss: 0.5199,\tval_loss: 1.9310\n",
      "111:\t[0s / 11s],\t\ttrain_loss: 0.5194,\tval_loss: 1.9258\n",
      "112:\t[0s / 11s],\t\ttrain_loss: 0.5174,\tval_loss: 1.9206\n",
      "113:\t[0s / 11s],\t\ttrain_loss: 0.5162,\tval_loss: 1.9158\n",
      "114:\t[0s / 11s],\t\ttrain_loss: 0.5179,\tval_loss: 1.9107\n",
      "115:\t[0s / 11s],\t\ttrain_loss: 0.5172,\tval_loss: 1.9058\n",
      "116:\t[0s / 11s],\t\ttrain_loss: 0.5161,\tval_loss: 1.9007\n",
      "117:\t[0s / 11s],\t\ttrain_loss: 0.5155,\tval_loss: 1.8964\n",
      "118:\t[0s / 11s],\t\ttrain_loss: 0.5127,\tval_loss: 1.8928\n",
      "119:\t[0s / 11s],\t\ttrain_loss: 0.5162,\tval_loss: 1.8889\n",
      "120:\t[0s / 12s],\t\ttrain_loss: 0.5139,\tval_loss: 1.8837\n",
      "121:\t[0s / 12s],\t\ttrain_loss: 0.5127,\tval_loss: 1.8790\n",
      "122:\t[0s / 12s],\t\ttrain_loss: 0.5155,\tval_loss: 1.8742\n",
      "123:\t[0s / 12s],\t\ttrain_loss: 0.5126,\tval_loss: 1.8694\n",
      "124:\t[0s / 12s],\t\ttrain_loss: 0.5129,\tval_loss: 1.8648\n",
      "125:\t[0s / 12s],\t\ttrain_loss: 0.5135,\tval_loss: 1.8603\n",
      "126:\t[0s / 12s],\t\ttrain_loss: 0.5116,\tval_loss: 1.8558\n",
      "127:\t[0s / 12s],\t\ttrain_loss: 0.5106,\tval_loss: 1.8515\n",
      "128:\t[0s / 12s],\t\ttrain_loss: 0.5122,\tval_loss: 1.8473\n",
      "129:\t[0s / 12s],\t\ttrain_loss: 0.5114,\tval_loss: 1.8442\n",
      "130:\t[0s / 12s],\t\ttrain_loss: 0.5080,\tval_loss: 1.8412\n",
      "131:\t[0s / 12s],\t\ttrain_loss: 0.5094,\tval_loss: 1.8379\n",
      "132:\t[0s / 12s],\t\ttrain_loss: 0.5087,\tval_loss: 1.8340\n",
      "133:\t[0s / 12s],\t\ttrain_loss: 0.5096,\tval_loss: 1.8309\n",
      "134:\t[0s / 12s],\t\ttrain_loss: 0.5083,\tval_loss: 1.8277\n",
      "135:\t[0s / 12s],\t\ttrain_loss: 0.5086,\tval_loss: 1.8248\n",
      "136:\t[0s / 13s],\t\ttrain_loss: 0.5059,\tval_loss: 1.8215\n",
      "137:\t[0s / 13s],\t\ttrain_loss: 0.5052,\tval_loss: 1.8186\n",
      "138:\t[0s / 13s],\t\ttrain_loss: 0.5075,\tval_loss: 1.8157\n",
      "139:\t[0s / 13s],\t\ttrain_loss: 0.5065,\tval_loss: 1.8126\n",
      "140:\t[0s / 13s],\t\ttrain_loss: 0.5082,\tval_loss: 1.8093\n",
      "141:\t[0s / 13s],\t\ttrain_loss: 0.5074,\tval_loss: 1.8070\n",
      "142:\t[0s / 13s],\t\ttrain_loss: 0.5072,\tval_loss: 1.8054\n",
      "143:\t[0s / 13s],\t\ttrain_loss: 0.5078,\tval_loss: 1.8034\n",
      "144:\t[0s / 13s],\t\ttrain_loss: 0.5064,\tval_loss: 1.7997\n",
      "145:\t[0s / 13s],\t\ttrain_loss: 0.5056,\tval_loss: 1.7974\n",
      "146:\t[0s / 13s],\t\ttrain_loss: 0.5051,\tval_loss: 1.7948\n",
      "147:\t[0s / 13s],\t\ttrain_loss: 0.5044,\tval_loss: 1.7910\n",
      "148:\t[0s / 13s],\t\ttrain_loss: 0.5033,\tval_loss: 1.7886\n",
      "149:\t[0s / 13s],\t\ttrain_loss: 0.5030,\tval_loss: 1.7860\n",
      "150:\t[0s / 13s],\t\ttrain_loss: 0.5046,\tval_loss: 1.7841\n",
      "151:\t[0s / 13s],\t\ttrain_loss: 0.5041,\tval_loss: 1.7818\n",
      "152:\t[0s / 14s],\t\ttrain_loss: 0.5032,\tval_loss: 1.7792\n",
      "153:\t[0s / 14s],\t\ttrain_loss: 0.5032,\tval_loss: 1.7770\n",
      "154:\t[0s / 14s],\t\ttrain_loss: 0.5024,\tval_loss: 1.7751\n",
      "155:\t[0s / 14s],\t\ttrain_loss: 0.5006,\tval_loss: 1.7726\n",
      "156:\t[0s / 14s],\t\ttrain_loss: 0.4998,\tval_loss: 1.7719\n",
      "157:\t[0s / 14s],\t\ttrain_loss: 0.5019,\tval_loss: 1.7703\n",
      "158:\t[0s / 14s],\t\ttrain_loss: 0.5011,\tval_loss: 1.7674\n",
      "159:\t[0s / 14s],\t\ttrain_loss: 0.5039,\tval_loss: 1.7648\n",
      "160:\t[0s / 14s],\t\ttrain_loss: 0.5023,\tval_loss: 1.7628\n",
      "161:\t[0s / 14s],\t\ttrain_loss: 0.5005,\tval_loss: 1.7609\n",
      "162:\t[0s / 14s],\t\ttrain_loss: 0.5017,\tval_loss: 1.7604\n",
      "163:\t[0s / 14s],\t\ttrain_loss: 0.5006,\tval_loss: 1.7588\n",
      "164:\t[0s / 14s],\t\ttrain_loss: 0.4982,\tval_loss: 1.7563\n",
      "165:\t[0s / 14s],\t\ttrain_loss: 0.4999,\tval_loss: 1.7547\n",
      "166:\t[0s / 14s],\t\ttrain_loss: 0.4995,\tval_loss: 1.7542\n",
      "167:\t[0s / 14s],\t\ttrain_loss: 0.5001,\tval_loss: 1.7532\n",
      "168:\t[0s / 15s],\t\ttrain_loss: 0.4975,\tval_loss: 1.7533\n",
      "169:\t[0s / 15s],\t\ttrain_loss: 0.5035,\tval_loss: 1.7523\n",
      "170:\t[0s / 15s],\t\ttrain_loss: 0.5005,\tval_loss: 1.7484\n",
      "171:\t[0s / 15s],\t\ttrain_loss: 0.4968,\tval_loss: 1.7481\n",
      "172:\t[0s / 15s],\t\ttrain_loss: 0.4954,\tval_loss: 1.7463\n",
      "173:\t[0s / 15s],\t\ttrain_loss: 0.4967,\tval_loss: 1.7458\n",
      "174:\t[0s / 15s],\t\ttrain_loss: 0.4964,\tval_loss: 1.7441\n",
      "175:\t[0s / 15s],\t\ttrain_loss: 0.4966,\tval_loss: 1.7424\n",
      "176:\t[0s / 15s],\t\ttrain_loss: 0.4951,\tval_loss: 1.7418\n",
      "177:\t[0s / 15s],\t\ttrain_loss: 0.4968,\tval_loss: 1.7403\n",
      "178:\t[0s / 15s],\t\ttrain_loss: 0.4983,\tval_loss: 1.7388\n",
      "179:\t[0s / 15s],\t\ttrain_loss: 0.4949,\tval_loss: 1.7375\n",
      "180:\t[0s / 15s],\t\ttrain_loss: 0.4970,\tval_loss: 1.7347\n",
      "181:\t[0s / 15s],\t\ttrain_loss: 0.4957,\tval_loss: 1.7338\n",
      "182:\t[0s / 15s],\t\ttrain_loss: 0.4960,\tval_loss: 1.7337\n",
      "183:\t[0s / 15s],\t\ttrain_loss: 0.4956,\tval_loss: 1.7326\n",
      "184:\t[0s / 16s],\t\ttrain_loss: 0.4939,\tval_loss: 1.7299\n",
      "185:\t[0s / 16s],\t\ttrain_loss: 0.4943,\tval_loss: 1.7272\n",
      "186:\t[0s / 16s],\t\ttrain_loss: 0.4962,\tval_loss: 1.7251\n",
      "187:\t[0s / 16s],\t\ttrain_loss: 0.4939,\tval_loss: 1.7230\n",
      "188:\t[0s / 16s],\t\ttrain_loss: 0.4932,\tval_loss: 1.7224\n",
      "189:\t[0s / 16s],\t\ttrain_loss: 0.4932,\tval_loss: 1.7204\n",
      "190:\t[0s / 16s],\t\ttrain_loss: 0.4929,\tval_loss: 1.7187\n",
      "191:\t[0s / 16s],\t\ttrain_loss: 0.4957,\tval_loss: 1.7173\n",
      "192:\t[0s / 16s],\t\ttrain_loss: 0.4908,\tval_loss: 1.7158\n",
      "193:\t[0s / 16s],\t\ttrain_loss: 0.4929,\tval_loss: 1.7132\n",
      "194:\t[0s / 16s],\t\ttrain_loss: 0.4939,\tval_loss: 1.7120\n",
      "195:\t[0s / 16s],\t\ttrain_loss: 0.4958,\tval_loss: 1.7110\n",
      "196:\t[0s / 16s],\t\ttrain_loss: 0.4917,\tval_loss: 1.7115\n",
      "197:\t[0s / 16s],\t\ttrain_loss: 0.4920,\tval_loss: 1.7103\n",
      "198:\t[0s / 16s],\t\ttrain_loss: 0.4929,\tval_loss: 1.7091\n",
      "199:\t[0s / 16s],\t\ttrain_loss: 0.4920,\tval_loss: 1.7075\n",
      "200:\t[0s / 17s],\t\ttrain_loss: 0.4903,\tval_loss: 1.7085\n",
      "201:\t[0s / 17s],\t\ttrain_loss: 0.4920,\tval_loss: 1.7096\n",
      "202:\t[0s / 17s],\t\ttrain_loss: 0.4929,\tval_loss: 1.7087\n",
      "203:\t[0s / 17s],\t\ttrain_loss: 0.4916,\tval_loss: 1.7064\n",
      "204:\t[0s / 17s],\t\ttrain_loss: 0.4907,\tval_loss: 1.7065\n",
      "205:\t[0s / 17s],\t\ttrain_loss: 0.4900,\tval_loss: 1.7062\n",
      "206:\t[0s / 17s],\t\ttrain_loss: 0.4896,\tval_loss: 1.7057\n",
      "207:\t[0s / 17s],\t\ttrain_loss: 0.4916,\tval_loss: 1.7052\n",
      "208:\t[0s / 17s],\t\ttrain_loss: 0.4894,\tval_loss: 1.7057\n",
      "209:\t[0s / 17s],\t\ttrain_loss: 0.4893,\tval_loss: 1.7056\n",
      "210:\t[0s / 17s],\t\ttrain_loss: 0.4907,\tval_loss: 1.7046\n",
      "211:\t[0s / 17s],\t\ttrain_loss: 0.4889,\tval_loss: 1.7042\n",
      "212:\t[0s / 17s],\t\ttrain_loss: 0.4877,\tval_loss: 1.7050\n",
      "213:\t[0s / 17s],\t\ttrain_loss: 0.4896,\tval_loss: 1.7057\n",
      "214:\t[0s / 17s],\t\ttrain_loss: 0.4871,\tval_loss: 1.7060\n",
      "215:\t[0s / 17s],\t\ttrain_loss: 0.4887,\tval_loss: 1.7050\n",
      "216:\t[0s / 17s],\t\ttrain_loss: 0.4888,\tval_loss: 1.7049\n",
      "217:\t[0s / 17s],\t\ttrain_loss: 0.4882,\tval_loss: 1.7034\n",
      "218:\t[0s / 18s],\t\ttrain_loss: 0.4874,\tval_loss: 1.7036\n",
      "219:\t[0s / 18s],\t\ttrain_loss: 0.4901,\tval_loss: 1.7020\n",
      "220:\t[0s / 18s],\t\ttrain_loss: 0.4884,\tval_loss: 1.7018\n",
      "221:\t[0s / 18s],\t\ttrain_loss: 0.4881,\tval_loss: 1.7004\n",
      "222:\t[0s / 18s],\t\ttrain_loss: 0.4872,\tval_loss: 1.7006\n",
      "223:\t[0s / 18s],\t\ttrain_loss: 0.4870,\tval_loss: 1.6995\n",
      "224:\t[0s / 18s],\t\ttrain_loss: 0.4867,\tval_loss: 1.7004\n",
      "225:\t[0s / 18s],\t\ttrain_loss: 0.4883,\tval_loss: 1.7022\n",
      "226:\t[0s / 18s],\t\ttrain_loss: 0.4861,\tval_loss: 1.7000\n",
      "227:\t[0s / 18s],\t\ttrain_loss: 0.4858,\tval_loss: 1.6990\n",
      "228:\t[0s / 18s],\t\ttrain_loss: 0.4853,\tval_loss: 1.6988\n",
      "229:\t[0s / 18s],\t\ttrain_loss: 0.4836,\tval_loss: 1.6979\n",
      "230:\t[0s / 18s],\t\ttrain_loss: 0.4854,\tval_loss: 1.6993\n",
      "231:\t[0s / 18s],\t\ttrain_loss: 0.4862,\tval_loss: 1.6996\n",
      "232:\t[0s / 18s],\t\ttrain_loss: 0.4857,\tval_loss: 1.6995\n",
      "233:\t[0s / 18s],\t\ttrain_loss: 0.4843,\tval_loss: 1.6980\n",
      "234:\t[0s / 18s],\t\ttrain_loss: 0.4846,\tval_loss: 1.6981\n",
      "235:\t[0s / 19s],\t\ttrain_loss: 0.4862,\tval_loss: 1.6989\n",
      "236:\t[0s / 19s],\t\ttrain_loss: 0.4871,\tval_loss: 1.6977\n",
      "237:\t[0s / 19s],\t\ttrain_loss: 0.4836,\tval_loss: 1.6971\n",
      "238:\t[0s / 19s],\t\ttrain_loss: 0.4848,\tval_loss: 1.6970\n",
      "239:\t[0s / 19s],\t\ttrain_loss: 0.4842,\tval_loss: 1.6974\n",
      "240:\t[0s / 19s],\t\ttrain_loss: 0.4826,\tval_loss: 1.6970\n",
      "241:\t[0s / 19s],\t\ttrain_loss: 0.4816,\tval_loss: 1.6960\n",
      "242:\t[0s / 19s],\t\ttrain_loss: 0.4836,\tval_loss: 1.6959\n",
      "243:\t[0s / 19s],\t\ttrain_loss: 0.4821,\tval_loss: 1.6959\n",
      "244:\t[0s / 19s],\t\ttrain_loss: 0.4830,\tval_loss: 1.6964\n",
      "245:\t[0s / 19s],\t\ttrain_loss: 0.4825,\tval_loss: 1.6966\n",
      "246:\t[0s / 19s],\t\ttrain_loss: 0.4827,\tval_loss: 1.6953\n",
      "247:\t[0s / 19s],\t\ttrain_loss: 0.4789,\tval_loss: 1.6951\n",
      "248:\t[0s / 19s],\t\ttrain_loss: 0.4834,\tval_loss: 1.6955\n",
      "249:\t[0s / 19s],\t\ttrain_loss: 0.4824,\tval_loss: 1.6954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torchtuples.callbacks.TrainingLogger at 0x1e97f729820>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pycox.models import DeepHitSingle\n",
    "import torchtuples as tt\n",
    "\n",
    "model_deephit = DeepHitSingle(model, torch.optim.Adam, alpha=0.2, sigma=0.1, duration_index=processor.cuts)\n",
    "\n",
    "model_deephit.fit(x_train, y_train, 256,250, val_data = (x_val, y_val), callbacks = [tt.callbacks.EarlyStopping()])\n",
    "# model_deephit.interpolate(10).predict_surv_df(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_risk = model_deephit.interpolate(10).predict_surv_df(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = EvalSurv(all_risk, durations_test, events_test, censor_surv='km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.655792954343126"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.concordance_td('antolini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dream_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
